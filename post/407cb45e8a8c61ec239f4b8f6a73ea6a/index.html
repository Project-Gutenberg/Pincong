<!doctype html><html lang=zh-cn><head><meta charset=utf-8><title>我不担心ChatGPT抢我饭碗，但我担心那件更要命的事</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content=" 收录于合集 #那个文明从这里走过 40个
中国人的下一代，会因它而提前过时么？
聊聊最近很火的ChatGPT的事儿 。
这两天，美国人开发的“会聊天的机器人”ChatGPT非常火爆，很多人惊叹于这款人工智能的“能侃”——给它个题目，它能很快回给你一篇小作文，且语言风格、叙述逻辑都不像之前的AI那样生硬了，粗略一聊，好像真的是一个人写的。
甚至还有人逗闷子，让ChatGPT模仿一些大V写稿。
比如要求ChatGPT在论述某命题作文时，每段前面都加上“老胡觉得”“老胡还觉得”。
很多人试过以后说，基本看不出它和正主写的有啥区别……
于是也有朋友跑我这儿来打趣，说：“小西，你担不担心ChatGPT再发展发展，把你饭碗也给抢了啊？”
对这种问题，我一般都回答“我不担心，但我很担心一些更要命的事。”
下面我就单开一篇文章，谈谈这个问题：
1
其实说起人工智能，我大学刚毕业那会儿曾经是这项技术的狂信徒，信仰到工作了很多年我都没学车——因为那会儿自动驾驶被炒的很火，我经常幻想，再过上几年，这项技术再来个重大突破。驾驶座就被智能AI接管了，那我费劲巴力的考驾照还有啥用？我又不擅长这个。
可是后来有次跟相关专业的一位朋友聊天，他一句话就改变了我的想法。
我那位朋友是这么说的：“小西，车还是要学啊，可预见的未来，AI永远不可能代替人类把方向盘——因为“电车难题”无法解决。”
我一听这话就醒悟了 ，乖乖的去把驾照给考了——虽然前后挂了六次才过。
电车难题是什么呢？给没听过的朋友再解释两句。
这个问题最早是由英国当代哲学家菲利帕.福特在讨论堕胎问题时首先提出的。它的经典表述是这样的：
说一辆电车在轨道上行驶，在它前面的轨道上被绑着五个人，可是电车刚好失控了，停不下来，眼看就要把这五个人给碾死。
这时，你刚好站在电车轨道的操纵杆旁边。只要你拉下操纵杆，列车就会切换到备用轨道上去。可是，备用轨道上也有人被绑着，不过只有一个人。
那么问题来了：这个操纵杆，你是拉还是不拉呢？
电车难题问世以来，引发很多伦理学和法理学的讨论。功利主义和自由主义在拉不拉操纵杆这个问题上争的面红耳赤，但这个问题我们全切放在一边，有机会再聊。这里只说一个争到最后各方都认的共识——那就是，人是要为自己的选择负责的。
在现实当中，如果有一个人面临类似的两难抉择——比如“你妈和你媳妇掉水里，你救谁？”无论他最终的选择是什么，他人或者他自己的良心一定会不自觉的追问：我为什么当时要这么选？他必须说出一个所以然来。才能说服自己过去这个坎。
所以从这角度讲，**人要为自己的选择负责，也只有人才可以为自己的选择负责。**很多高阶的判断问题，如果涉及到伦理、道德、审美，无法被自动化的原因，就是它其实怎么选都对，怎么选也都错，只有当人类面临那个场景，自己做这个判断时，属于他的、能够让他安心的“正确答案”才会被揭晓。
明晰了这一点，我们再去看自动驾驶这种人工智能代替人类的难点。就会发现它就卡在这里——再驾驶这个场景中 ，很多时候是会面临类似的电车难题的。比如，很多交通事故发生在车辆高速行使过程当中，前方突然出现人或者动物横穿马路。在这种情况下，很多司机会下意识的打方向盘，而结果有时会是车辆失速侧翻，他要躲避的那个人没事，但开车者和他的亲友车毁人亡。
那么好了，假如人工智能发展到可以完全替代人类进行汽车驾驶，需要为这种场景进行判断。请问这个判断程序怎么写？
人工智能也许可以通过模拟运算，算出作出规避动作后，马路上的行人幸存概率增加，而司机的幸存概率骤减。而如果直接撞过去，行人八成就没命了，但司机可以幸存。但人工智能真的可以代替驾驶者做“直接开过去”这种选择么？这个选择是要承担日后的法律、道德追问的。
如果允许人工智能代做判断，抛开它将违反机器人不得伤害人类的“阿西莫夫定律”不谈，它也相当于让驾驶者逃离了这个必做选择的场景。最终这类事件会产生一种“无人负责”的荒诞悖论。
这种悖论，甚至哪怕让驾驶者在事先预设好自己的偏好判断也无法规避。
比如一个人可以预先设想自己在高速行使中即便看到他人横穿马路也会为了自己保命而直接冲过去，可是事关临头，如果那个人是孕妇、老人、孩子，或者是他的家人、爱人，他的选择也许就不一样了。
很多事情，不事关临头，人类就是不知道自己的选择会是什么。这就是人性。
人工智能永远无法复制人性的根本原因，不在于它能否深度模仿人类的思维，而在于它不能代替人进行这种判断。
由此我们想到《圣经》里那个故事——亚当和夏娃，因为偷吃禁果而受了神的厌弃与恐惧，被赶出了伊甸园 ，成为了真正的人。
但这个“禁果”是什么呢？《圣经》的原文当中管它叫“知善恶果”，但有些翻译当中，也会将其称为“智慧果”。
其实你仔细想想，这两个意思是想通的——人类思维中最核心的东西，是什么？不就是知善恶、明是非、做判断么？当人 有能力作出一个“怎么选都对，怎么选都错”的价值判断 ，并为这个判断负责时，他才是个人。
而就像《圣经》中的上帝不想让人具有这种自主意识一样，人类一定会避免人工智能拥有这个能力，否则就真的要上演《我，机器人》了。
于是我们得到结论，那些要做价值判断的工作，人类永远无法放手让机器人代劳。
开车如此 ，写文章就更如是了。
2
实不相瞒，为了写这篇稿子，我还真的专门去跟ChatGPT聊了聊，聊的过程中我也在反思——作为一个码字者，我写一篇文章，它对读者价值究竟在哪里？这些价值有可能被ChatGPT这种AI代替么？
反思后我发现，一篇文章，至少有三层价值。
第一是“知识价值”，也就是谈一件事，我要告诉你这件事是怎么回事、相关有什么数据、史料，把这些东西打包汇总给读者，给读者以开拓视野的作用。 "><meta name=generator content="Hugo 0.100.2"><link rel=stylesheet href=../../plugins/bootstrap/bootstrap.min.css><link rel=stylesheet href=../../plugins/themify-icons/themify-icons.css><link rel=stylesheet href=https://project-gutenberg.github.io/Pincong/scss/style.min.css media=screen><link rel="shortcut icon" href=https://project-gutenberg.github.io/Pincong/images/favicon.png type=image/x-icon><link rel=icon href=https://project-gutenberg.github.io/Pincong/images/favicon.png type=image/x-icon><meta name=twitter:card content="summary_large_image"><meta name=og:title content=" 我不担心ChatGPT抢我饭碗，但我担心那件更要命的事 "><meta name=og:description content=" 收录于合集 #那个文明从这里走过 40个
中国人的下一代，会因它而提前过时么？
聊聊最近很火的ChatGPT的事儿 。
这两天，美国人开发的“会聊天的机器人”ChatGPT非常火爆，很多人惊叹于这款人工智能的“能侃”——给它个题目，它能很快回给你一篇小作文，且语言风格、叙述逻辑都不像之前的AI那样生硬了，粗略一聊，好像真的是一个人写的。
甚至还有人逗闷子，让ChatGPT模仿一些大V写稿。
比如要求ChatGPT在论述某命题作文时，每段前面都加上“老胡觉得”“老胡还觉得”。
很多人试过以后说，基本看不出它和正主写的有啥区别……
于是也有朋友跑我这儿来打趣，说：“小西，你担不担心ChatGPT再发展发展，把你饭碗也给抢了啊？”
对这种问题，我一般都回答“我不担心，但我很担心一些更要命的事。”
下面我就单开一篇文章，谈谈这个问题：
1
其实说起人工智能，我大学刚毕业那会儿曾经是这项技术的狂信徒，信仰到工作了很多年我都没学车——因为那会儿自动驾驶被炒的很火，我经常幻想，再过上几年，这项技术再来个重大突破。驾驶座就被智能AI接管了，那我费劲巴力的考驾照还有啥用？我又不擅长这个。
可是后来有次跟相关专业的一位朋友聊天，他一句话就改变了我的想法。
我那位朋友是这么说的：“小西，车还是要学啊，可预见的未来，AI永远不可能代替人类把方向盘——因为“电车难题”无法解决。”
我一听这话就醒悟了 ，乖乖的去把驾照给考了——虽然前后挂了六次才过。
电车难题是什么呢？给没听过的朋友再解释两句。
这个问题最早是由英国当代哲学家菲利帕.福特在讨论堕胎问题时首先提出的。它的经典表述是这样的：
说一辆电车在轨道上行驶，在它前面的轨道上被绑着五个人，可是电车刚好失控了，停不下来，眼看就要把这五个人给碾死。
这时，你刚好站在电车轨道的操纵杆旁边。只要你拉下操纵杆，列车就会切换到备用轨道上去。可是，备用轨道上也有人被绑着，不过只有一个人。
那么问题来了：这个操纵杆，你是拉还是不拉呢？
电车难题问世以来，引发很多伦理学和法理学的讨论。功利主义和自由主义在拉不拉操纵杆这个问题上争的面红耳赤，但这个问题我们全切放在一边，有机会再聊。这里只说一个争到最后各方都认的共识——那就是，人是要为自己的选择负责的。
在现实当中，如果有一个人面临类似的两难抉择——比如“你妈和你媳妇掉水里，你救谁？”无论他最终的选择是什么，他人或者他自己的良心一定会不自觉的追问：我为什么当时要这么选？他必须说出一个所以然来。才能说服自己过去这个坎。
所以从这角度讲，**人要为自己的选择负责，也只有人才可以为自己的选择负责。**很多高阶的判断问题，如果涉及到伦理、道德、审美，无法被自动化的原因，就是它其实怎么选都对，怎么选也都错，只有当人类面临那个场景，自己做这个判断时，属于他的、能够让他安心的“正确答案”才会被揭晓。
明晰了这一点，我们再去看自动驾驶这种人工智能代替人类的难点。就会发现它就卡在这里——再驾驶这个场景中 ，很多时候是会面临类似的电车难题的。比如，很多交通事故发生在车辆高速行使过程当中，前方突然出现人或者动物横穿马路。在这种情况下，很多司机会下意识的打方向盘，而结果有时会是车辆失速侧翻，他要躲避的那个人没事，但开车者和他的亲友车毁人亡。
那么好了，假如人工智能发展到可以完全替代人类进行汽车驾驶，需要为这种场景进行判断。请问这个判断程序怎么写？
人工智能也许可以通过模拟运算，算出作出规避动作后，马路上的行人幸存概率增加，而司机的幸存概率骤减。而如果直接撞过去，行人八成就没命了，但司机可以幸存。但人工智能真的可以代替驾驶者做“直接开过去”这种选择么？这个选择是要承担日后的法律、道德追问的。
如果允许人工智能代做判断，抛开它将违反机器人不得伤害人类的“阿西莫夫定律”不谈，它也相当于让驾驶者逃离了这个必做选择的场景。最终这类事件会产生一种“无人负责”的荒诞悖论。
这种悖论，甚至哪怕让驾驶者在事先预设好自己的偏好判断也无法规避。
比如一个人可以预先设想自己在高速行使中即便看到他人横穿马路也会为了自己保命而直接冲过去，可是事关临头，如果那个人是孕妇、老人、孩子，或者是他的家人、爱人，他的选择也许就不一样了。
很多事情，不事关临头，人类就是不知道自己的选择会是什么。这就是人性。
人工智能永远无法复制人性的根本原因，不在于它能否深度模仿人类的思维，而在于它不能代替人进行这种判断。
由此我们想到《圣经》里那个故事——亚当和夏娃，因为偷吃禁果而受了神的厌弃与恐惧，被赶出了伊甸园 ，成为了真正的人。
但这个“禁果”是什么呢？《圣经》的原文当中管它叫“知善恶果”，但有些翻译当中，也会将其称为“智慧果”。
其实你仔细想想，这两个意思是想通的——人类思维中最核心的东西，是什么？不就是知善恶、明是非、做判断么？当人 有能力作出一个“怎么选都对，怎么选都错”的价值判断 ，并为这个判断负责时，他才是个人。
而就像《圣经》中的上帝不想让人具有这种自主意识一样，人类一定会避免人工智能拥有这个能力，否则就真的要上演《我，机器人》了。
于是我们得到结论，那些要做价值判断的工作，人类永远无法放手让机器人代劳。
开车如此 ，写文章就更如是了。
2
实不相瞒，为了写这篇稿子，我还真的专门去跟ChatGPT聊了聊，聊的过程中我也在反思——作为一个码字者，我写一篇文章，它对读者价值究竟在哪里？这些价值有可能被ChatGPT这种AI代替么？
反思后我发现，一篇文章，至少有三层价值。
第一是“知识价值”，也就是谈一件事，我要告诉你这件事是怎么回事、相关有什么数据、史料，把这些东西打包汇总给读者，给读者以开拓视野的作用。 "><meta name=og:image content="https://project-gutenberg.github.io/Pincong//images/card/230.jpg"><script data-ad-client=ca-pub-6074407261372769 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-06HJ1E5XNH"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-06HJ1E5XNH')</script></head><body><header class="fixed-top navigation"><div class=container><nav class="navbar navbar-expand-lg navbar-light bg-transparent"><a class=navbar-brand href=https://project-gutenberg.github.io/Pincong/><img class=img-fluid src=https://project-gutenberg.github.io/Pincong//images/logo.png alt=品葱*精选></a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation>
<i class="ti-menu h3"></i></button><div class="collapse navbar-collapse text-center" id=navigation><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=https://project-gutenberg.github.io/Pincong/>Home</a></li><li class=nav-item><a class=nav-link href=https://project-gutenberg.github.io/Pincong/post/index.xml>RSS</a></li><li class=nav-item><a class=nav-link href=https://bit.ly/2HrxEi0>Telegram</a></li><li class=nav-item><a class=nav-link href=https://twitter.com/speechfree3>Twitter</a></li></ul></div></nav></div></header><div class="py-5 d-none d-lg-block"></div><section class=main-content><div class=container><div class=row><div class="col-lg-8 mx-auto block shadow mb-5"><h2>我不担心ChatGPT抢我饭碗，但我担心那件更要命的事</h2><div class=mb-3><span>by <a href=https://project-gutenberg.github.io/Pincong/author/%e6%b5%b7%e8%be%b9%e7%9a%84%e8%a5%bf%e5%a1%9e%e7%bd%97>海边的西塞罗</a></span>,
<span>at 11 February 2023</span>, tags :
<a href=https://project-gutenberg.github.io/Pincong/tags/%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd>人工智能</a>
<a href=https://project-gutenberg.github.io/Pincong/tags/%e5%88%a4%e6%96%ad>判断</a>
<a href=https://project-gutenberg.github.io/Pincong/tags/%e7%94%b5%e8%bd%a6>电车</a>
<a href=https://github.com/Project-Gutenberg/Pincong-data/edit/master/site/content/post/407cb45e8a8c61ec239f4b8f6a73ea6a.md style=color:gray><strong><i>点击纠错</i></strong></a>
<i></i><a href=https://github.com/Project-Gutenberg/Pincong-data/delete/master/site/content/post/407cb45e8a8c61ec239f4b8f6a73ea6a.md style=color:gray><strong><i>点击删除</i></strong></a></div><h5><a href=https://bit.ly/justmysock>使用CN2/CN2GIA顶级线路，支持Shadowsocks/V2ray科学上网，支持支付宝付款，每月仅需 5 美元</a></h5><h5><a href=https://bit.ly/2HrxEi0>## 加入品葱精选 Telegram Channel ##</a></h5><p></p><p>收录于合集 #那个文明从这里走过 40个</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_jpg/E3R2kpnIkmPp1xZfox1s0gfebIQiakIeaoklicZUiccEUpAZNiafCjRXwNPYhrxJGibZD9GmZC5urtgEFA4WuuHnPdA/640%3Fwx_fmt%3Djpeg" alt></p><p>中国人的下一代，会因它而提前过时么？</p><p>聊聊最近很火的ChatGPT的事儿 。</p><p>这两天，美国人开发的“会聊天的机器人”ChatGPT非常火爆，很多人惊叹于这款人工智能的“能侃”——给它个题目，它能很快回给你一篇小作文，且语言风格、叙述逻辑都不像之前的AI那样生硬了，粗略一聊，好像真的是一个人写的。</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_jpg/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5lvRuQgKBibhHwpPy1GxaDc3DXicPVvZ4xG1Ua0qml1vc7ZWyyuWmyov3A/640%3Fwx_fmt%3Djpeg" alt></p><p>甚至还有人逗闷子，让ChatGPT模仿一些大V写稿。</p><p>比如要求ChatGPT在论述某命题作文时，每段前面都加上“老胡觉得”“老胡还觉得”。</p><p>很多人试过以后说，基本看不出它和正主写的有啥区别……</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_png/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5l8ibbFSfgoL3y1cVBt5aJ6w6dtkvl2J9tFeic1hA0t1fDQcrwYG143ryA/640%3Fwx_fmt%3Dpng" alt></p><p>于是也有朋友跑我这儿来打趣，说：“小西，你担不担心ChatGPT再发展发展，把你饭碗也给抢了啊？”</p><p>对这种问题，我一般都回答“<strong>我不担心，但我很担心一些更要命的事。</strong>”</p><p>下面我就单开一篇文章，谈谈这个问题：</p><p>1</p><p>其实说起人工智能，我大学刚毕业那会儿曾经是这项技术的狂信徒，信仰到工作了很多年我都没学车——因为那会儿自动驾驶被炒的很火，我经常幻想，再过上几年，这项技术再来个重大突破。驾驶座就被智能AI接管了，那我费劲巴力的考驾照还有啥用？我又不擅长这个。</p><p>可是后来有次跟相关专业的一位朋友聊天，他一句话就改变了我的想法。</p><p>我那位朋友是这么说的：“小西，车还是要学啊，可预见的未来，AI永远不可能代替人类把方向盘——因为“<strong>电车难题</strong>”无法解决。”</p><p>我一听这话就醒悟了 ，乖乖的去把驾照给考了——虽然前后挂了六次才过。</p><p>电车难题是什么呢？给没听过的朋友再解释两句。</p><p>这个问题最早是由英国当代哲学家菲利帕.福特在讨论堕胎问题时首先提出的。它的经典表述是这样的：</p><p>说一辆电车在轨道上行驶，在它前面的轨道上被绑着五个人，可是电车刚好失控了，停不下来，眼看就要把这五个人给碾死。</p><p>这时，你刚好站在电车轨道的操纵杆旁边。只要你拉下操纵杆，列车就会切换到备用轨道上去。可是，备用轨道上也有人被绑着，不过只有一个人。</p><p>那么问题来了：这个操纵杆，你是拉还是不拉呢？</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_jpg/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5lcw5jHQqL5kOGFU2x9SgxuDzrku318rS8gcCBldXRMHulibBJF841miaQ/640%3Fwx_fmt%3Djpeg" alt></p><p>电车难题问世以来，引发很多伦理学和法理学的讨论。功利主义和自由主义在拉不拉操纵杆这个问题上争的面红耳赤，但这个问题我们全切放在一边，有机会再聊。这里只说一个争到最后各方都认的共识——那就是，人是要为自己的选择负责的。</p><p>在现实当中，如果有一个人面临类似的两难抉择——比如“你妈和你媳妇掉水里，你救谁？”无论他最终的选择是什么，他人或者他自己的良心一定会不自觉的追问：我为什么当时要这么选？他必须说出一个所以然来。才能说服自己过去这个坎。</p><p>所以从这角度讲，**人要为自己的选择负责，也只有人才可以为自己的选择负责。**很多高阶的判断问题，如果涉及到伦理、道德、审美，无法被自动化的原因，就是它其实怎么选都对，怎么选也都错，只有当人类面临那个场景，自己做这个判断时，属于他的、能够让他安心的“正确答案”才会被揭晓。</p><p>明晰了这一点，我们再去看自动驾驶这种人工智能代替人类的难点。就会发现它就卡在这里——再驾驶这个场景中 ，很多时候是会面临类似的电车难题的。比如，很多交通事故发生在车辆高速行使过程当中，前方突然出现人或者动物横穿马路。在这种情况下，很多司机会下意识的打方向盘，而结果有时会是车辆失速侧翻，他要躲避的那个人没事，但开车者和他的亲友车毁人亡。</p><p>那么好了，假如人工智能发展到可以完全替代人类进行汽车驾驶，需要为这种场景进行判断。<strong>请问这个判断程序怎么写？</strong></p><p>人工智能也许可以通过模拟运算，算出作出规避动作后，马路上的行人幸存概率增加，而司机的幸存概率骤减。而如果直接撞过去，行人八成就没命了，但司机可以幸存。但人工智能真的可以代替驾驶者做“直接开过去”这种选择么？这个选择是要承担日后的法律、道德追问的。</p><p>如果允许人工智能代做判断，抛开它将违反机器人不得伤害人类的“阿西莫夫定律”不谈，它也相当于让驾驶者逃离了这个必做选择的场景。最终这类事件会产生一种“无人负责”的荒诞悖论。</p><p>这种悖论，甚至哪怕让驾驶者在事先预设好自己的偏好判断也无法规避。</p><p>比如一个人可以预先设想自己在高速行使中即便看到他人横穿马路也会为了自己保命而直接冲过去，可是事关临头，如果那个人是孕妇、老人、孩子，或者是他的家人、爱人，他的选择也许就不一样了。</p><p>很多事情，不事关临头，人类就是不知道自己的选择会是什么。这就是人性。</p><p><strong>人工智能永远无法复制人性的根本原因，不在于它能否深度模仿人类的思维，而在于它不能代替人进行这种判断。</strong></p><p>由此我们想到《圣经》里那个故事——亚当和夏娃，因为偷吃禁果而受了神的厌弃与恐惧，被赶出了伊甸园 ，成为了真正的人。</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_png/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5lW7WibYDkiapSqxUwIuUbRmcnFSPaO6aKxUibA6vbvTkaEdiav4YibS2fPuw/640%3Fwx_fmt%3Dpng" alt></p><p>但这个“禁果”是什么呢？《圣经》的原文当中管它叫“知善恶果”，但有些翻译当中，也会将其称为“智慧果”。</p><p>其实你仔细想想，这两个意思是想通的——人类思维中最核心的东西，是什么？不就是知善恶、明是非、做判断么？当人 有能力作出一个“怎么选都对，怎么选都错”的价值判断 ，并为这个判断负责时，他才是个人。</p><p>而就像《圣经》中的上帝不想让人具有这种自主意识一样，人类一定会避免人工智能拥有这个能力，否则就真的要上演《我，机器人》了。</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_jpg/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5lib56nmqqhdLtUjj7gEiaTp9j3Y9GPy6BnJseoRbgKXOiasC38redUfdAA/640%3Fwx_fmt%3Djpeg" alt></p><p>于是我们得到结论，那些要做价值判断的工作，人类永远无法放手让机器人代劳。</p><p>开车如此 ，写文章就更如是了。</p><p>2</p><p>实不相瞒，为了写这篇稿子，我还真的专门去跟ChatGPT聊了聊，聊的过程中我也在反思——作为一个码字者，我写一篇文章，它对读者价值究竟在哪里？这些价值有可能被ChatGPT这种AI代替么？</p><p>反思后我发现，一篇文章，至少有三层价值。</p><p><strong>第一是“知识价值”</strong>，也就是谈一件事，我要告诉你这件事是怎么回事、相关有什么数据、史料，把这些东西打包汇总给读者，给读者以开拓视野的作用。</p><p>这一层的价值，ChatGPT确实已经可以部分做到了，聊天过程当中你问它一个什么什么事，它能这件事的前因后果很准确、客观且较有条理提供给你，这种能力甚至已经超越了人类写作者。</p><p>但我之所以说，即便提供知识价值，ChatGPT也只能部分做到，是因为我发现它尚没有的“深度联想”能力——谈一件事，它就只能给你就事论事的说这件事，像这篇文章那样，从人工智能聊到电车难题、再想到《圣经》，这种开脑洞的创新性联想，它是不具备的。</p><p>于是你就会发现一个问题——跟这家伙聊天不“有趣”，人类思维的有趣，其实是建立在对多种知识的“通感”之上的。</p><p>“绿杨烟外晓寒轻，红杏枝头春意闹。”“桃花潭水深深千尺，不及汪伦送我情。”“春意”和“闹”之间，友情和水深之间，本来是没有逻辑联系的，但人类的思维就是能通过这种合理的“大脑短路”将其构建联系。我总认为这就是思维乐趣。</p><p>而这样的知识放飞自我又合情合理的铺展、蔓延，我在ChatGPT写的文章中是看不到的，AI能否通过深度学习或算法演进达成这种“通感”式的联想，我对此持保留态度。</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_png/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5lIIF9AOdoSGJ3tyGc8yMibGFOQs6PHNz6vicYP0ic7VicAmorkSV01I6ofg/640%3Fwx_fmt%3Dpng" alt></p><p><strong>第二是“情绪价值”</strong>，读我文章常了的朋友会知道，我写文章是喜欢做情绪表达的，评论事情并非单纯的平铺直叙，有些事让我开心，有些事引我调侃，有些事激我愤怒，有些事令我沉思。嬉笑怒骂之于文章，恰似酸甜苦辣之于菜肴。光有知识没有情绪的文章，如同只有营养没有味道的菜，至少我是不喜欢读的。</p><p>而情绪这个东西，目前的人工智能还很难模仿，即便能进行模仿，跟ChatGPT这种机器人聊天，那种感觉也有点像跟硅胶娃娃谈恋爱——她可能各方面都能满足你的需求，但归根结底，你会觉得味道是不对的，因为你知道对面那个东西，是个没有生命的死物，它的情绪只是用一堆机械或计算架构模拟出来的。在新奇感消失之后，就什么都没有了。</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_jpg/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5lJU0tnS7sanE7B7XvfwX7UEFibicxEr5KDpln3pjDeAU6hvgibUeMwWxxQ/640%3Fwx_fmt%3Djpeg" alt></p><p>谈到这里，我们不得不解析一个问题：<strong>情绪这种东西的本质是什么？</strong></p><p>在我看来，它其实也是一种人类才能作出的取舍和判断。</p><p>因为与知识的联想与通感需要跨界调动不同的知识体系刚好相反，情绪的产生与共鸣，其实恰恰来源于我们选择性的关闭一部分认知甚至常识。</p><p>比如愤怒，《满江红》的名句“壮士饥餐胡虏肉，笑谈渴饮匈奴血。”读来让人很提气。但如果你真的较真，吃人肉、喝人血这种事情是很践踏常识、甚至恐怖的事情，这么写和儒家痛斥的“率兽食人”的区别究竟在哪里呢？作者为什么在这里可以屏蔽这部分常识，却依然让人共情呢？</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_png/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5lMa7adJqicoFNL7G9ibD4y7a3m4nOUIs0LsalwV60ia0hGnSLAMcNnYEoQ/640%3Fwx_fmt%3Dpng" alt></p><p>这就说明人类在作出“愤怒”这个简单动作的背后，做了非常复杂的伦理取舍与判断，我们选择性的关闭了一些认知。于是愤怒的情绪才能表达出来，并激发人共情。</p><p>同样认知屏蔽，在“戏谑”、“悲伤”、“快乐”当中也会发生，AI无法自主模仿和使用这些情绪，与它无法处理“电车难题”一样，也是因为它无法主动选择在什么时候关闭什么认知。</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_jpg/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5l8rXibc6CkEp56nqmyiadAq5FWDxamoPU9yyTej9IU2AJtjaFseYBRyDw/640%3Fwx_fmt%3Djpeg" alt></p><p>它只能在你让它讲笑话时讲个笑话、让它表现愤怒时愤怒，让它唱首赞歌的时候唱赞歌。</p><p>于是我们就讨论到了文章价值中最重要的第三层：<strong>“判断价值”</strong>。</p><p>是的，对于一篇文章，判断是有价值的，而且是最高的价值。在跟ChatGPT聊天的时候，你会发现一个有趣的现象：一旦你问它如何评价一些敏感的历史人物，或者怎样评价“少数族裔”“同性恋”“堕胎”等问题时，该AI立刻丢弃原本尽力模仿人类聊天的那种风格，告诉你的本AI只是聊天工具，无法对该类争议问题作出评述。</p><p>在我看来，ChatGPT这样说，并不是开发者为了明哲保身，故意不谈敏感话题。而是因为这类话题谈到最后，往往都涉及“电车难题”式的取舍与判断，所以无法交给AI去做 。</p><p>比如谈到堕胎，你是尊重母亲的选择权还是婴儿的生命权？谈到英雄，你是赞赏他对历史车轮的推动，还是看重那些被车轮碾碎的个体？谈到经济模式，面对同样的经济困局和数据，你是赞同凯恩斯还是哈耶克？……</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_png/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5licyjHoqCpYmtzsfmvqlYjkCQ53OEFZYeOayxhoOdqKP49nB7KI1Rr5w/640%3Fwx_fmt%3Dpng" alt></p><p>凡此种种，想说出个所以然，你必须首先把一些“电车难题”式判断做了。而这一点，恰恰是AI没有办法进行的，如前所述，除非AI真的像伊甸园的亚当与夏娃一样获得了知善恶、行判断的能力，否则它永远无法写这样的文章。而如果AI达成这一点，就意味着它获得了自主意识。那我们的问题，可就不是司机或作家会不会被砸饭碗这么简单了。</p><p>综上所述：</p><p>在知识层面，AI 暂时不具备将不同体系知识进行通感和创新式的能力。</p><p>在情绪层面，AI很难模仿人类准确把握在什么时机关闭何种认知以达成哪种情绪。</p><p>而在判断层面，在我们需要操心的未来，AI因为不能具有人格，无法进行复杂的价值判断。</p><p>AI写作注定只能写这三种东西都没有的文章。ChatGPT的写作能力，其实已经接近触及这类文章的天花板了。</p><p>所以，就像我对坐在汽车后座上看人工智能替我开车已经断念一样，我同样永远不担心AI会砸我的写作饭碗——<strong>本质上讲，我和它从事的压根不是一个行当。</strong></p><p>3</p><p>但是，虽然我并不担心ChatGPT会砸我的饭碗，但在和它聊过之后，我却觉得另一种我们很仰赖的东西，很可能会随着人工智能这场变革而过时、落伍——<strong>那就是我们的文化曾引以为傲的教育方式，和这种教育方式下流水线般生产出来的人才。</strong></p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_jpg/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5lVdrJz65wNSHpAFVsqrVqu2U0gjz4WprQyb4hcxrsTMWX7R8ibxfFvKg/640%3Fwx_fmt%3Djpeg" alt></p><p>你小时候有没有过这样的经历？逢年过节，亲戚朋友聚在一起夸耀自己的孩子，兴致所至，会把孩子叫到跟前，让他给叔叔阿姨表演一个节目。而节目一般总是这样几类：识字或朗诵个唐诗宋词，算数或者背个九九乘法表和圆周率，弹钢琴、拉小提琴、吹个长笛黑管。</p><p>这样的表演和孩子们之间的竞赛，我小时候都经历过，而且还算是这类“鸡娃内卷”中的佼佼者。但你回头仔细想想，**我们的父母愿意为之自豪的这些“特长”，其实都是高度“技术性”的：**你能背下多少诗词、能进行多么精妙准确的数字运算、以及对乐器的肌肉记忆准确到了什么程度，能不能把乐曲弹奏的“就跟电视里放的一样”……</p><p>而推而广之的说，当我们走进学校，所经历的整个教育训练以及与之配套的选拔标准，也是这种技术性的，于是这就让我们的学生一度看起来非常牛逼，大则奥赛拿奖，小也能当个“做题家”，把高考的分数弄得高高的。</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_jpg/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5lgzS6uicUmzTFBZ4sp55ibPFMo9bjjFHFP2WOv99oGDGLWKMuKQ67E0lg/640%3Fwx_fmt%3Djpeg" alt></p><p>但现在的问题是，在技术的日新月异下，这一类技能人已经越来越干不过机器了——</p><p>唐诗宋词、圆周率你记得再熟，赶得上手机百度一下靠谱么？</p><p>四则运算，甚至徒手开平方你算的再快，赶得上掏出计算机直接算一下快么？</p><p>甚至你拎把琴到大街上去拉一曲，如果无法融入自己的情感和演艺，你拼的过帕尔曼、吕思清、帕格尼尼么？</p><p>所以过度注重技术性技能的培训，其实是一种前信息时代、甚至前工业时代的培训方式，在物资和信息都很匮乏的农耕社会，人们不得不靠死记硬背去记忆一些典籍，去徒手进行一些运算。可是到了信息时代，尤其是人工智能发展起来后，这些“<strong>体力劳动式的脑力劳动</strong>”，都正在迅速被取代和淘汰。</p><p>所以我们的很多教育，很可能是在让孩子苦练和比拼一些已经过时的东西。</p><p>明年春节，你让自家孩子在亲戚面前表演个钻木取火吧。其实这和你让他背圆周率啥的是一样的——反正你耍的再花哨，将来都一样是用不上。</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_gif/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5lAWOa0Eotr2SEjllr0PMBcMFdcnmuriaOUpSv3cIWHbFNonUyUVZezIg/640%3Fwx_fmt%3Dgif" alt></p><p>那么真正信息时代的教育应该是怎样的，我有一位同学，在美国一路读到博士。有一次我跟他聊天，他说他刚去时最毁三观的事情，就是老美太“傻x”，理科考试居然让带计算器进场，碰上个小学乘法，咱拿九九乘法表口算，他们在那儿啪啪摁计算器。文科老师也很拉胯，上课跟学生海聊到一个知识点，具体数字咬不准，毫不避讳的去办公室翻本书来现查。</p><p>我问，那你觉得他们好的地方在哪儿？他想了想说，美国顶级大学的讨论课比我们做的好。老师和学生读完书把观点放在一起碰，能聊出一些很不错的真东西来。不像咱很多学校，就是让学生轮流上去插优盘做个Presentation（报告）就交差了。</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_png/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5lf0ibs1L3REVw1kJ9ibSicDwxwo6mbCiba0S7raj7iafHHL0M2xdOg6uVl9g/640%3Fwx_fmt%3Dpng" alt></p><p>我曾经也没觉得这是个什么大问题，直到我后来在媒体工作，带实习生，有一次改他的稿子。</p><p>今天想来，他的文章写的就是那种“ChatGPT风格”——数据很准确、资料很详实、但通篇读下来，就是那么一种一点味道都没有的感觉——“听君一席话，如听一席话”。</p><p>我当时就说：</p><p>“你……这文章写的挺用心的，就是……能不能再加一点好玩的东西进去？<img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_png/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5lnKnOO3Wxiajia7tWmsbEZ8lsrOmA2tYdtJZQ85icaywCficbwp2s5ic7DvQ/640%3Fwx_fmt%3Dpng" alt>”</p><p>他很不解的问我：</p><p><strong>“西老师，什么叫“好玩的东西”？****<img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_png/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5l79ibxYnxqD29GEC8KWoAEJ7rvn6g3UPVNhibDV1InZMXoEEr3k5sKstg/640%3Fwx_fmt%3Dpng" alt>”</strong></p><p>我当时自己没想明白，没法回答，试了试，还是就让他那文章那么过了。</p><p>但我想，今天这篇文章，我总算把这事儿捋清楚了：</p><p><strong>在知识层面，就是那种“通感”式的新奇有趣的联想能力。</strong></p><p><strong>在情绪层面，就是那种蕴含其中、引而不发的“文气”。</strong></p><p><strong>在判断层面，就是那种未必一定正确、但能够给读者以启发或畅快感的见解。</strong></p><p>这些东西，是AI无法模仿的，而只有人肉码字的人才能写出来。</p><p>它是我们吃这碗饭的人的看家本领，也是你能用于躲避AI砸饭碗的一招鲜，也很可能是国外一些高等学府注重培养的，但却恰恰是我们的教育所不重视的。</p><p>我们的教育中摸爬滚打出来的好学生、做题家，大多可以非常优质的完成数据资料汇总这种“技术性 ”的工作——<strong>但这个工作，现在人工智能也能做了，而且能比你做的更快更好。</strong></p><p>我不是搞工科技术的，所以我不知道我们工业上“创新力不足”的问题是不是也和这有关系。这里就不赘述了。</p><p>总而言之，人工智能突飞猛进，对脑力劳动者们来说，<strong>其实不是一次毁灭一切“末日天劫”，而更像是一场“洪水”——当洪水袭来的时候，住在低地上的人们遭了殃，而处在高地上的人们却可以安然无恙。</strong></p><p><strong>现在我们看到的局势，就是洪水正在袭来，而发达国家正在通过它们的教育将其精英人才“搬到高处”。</strong></p><p>而我们所要担忧的问题其实是，我们文化过去过于注重的那种对“技术性”技能的培养和选拔，让我们的太多人才都成了“<strong>低地上的人</strong>”，洪水没有到来的时候，我们和人家一样过日子，甚至可以依靠人口众多和勤劳的精耕细作比他们做的更好。</p><p>可是，万一人工智能的洪水袭来，将低地淹没、高地留存，局势很可能会发生骤然的变化。</p><p>这是一场基于技术革命的“水淹七军”。</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_png/lYDNMgmFjur6mfkPywZNgKZYXia5C4lA8Kd6CUYOibay3HbIfxBnNXH3v6jQZf6QquSL5dt6qYMrtQGh7go67FZg/640%3Fwx_fmt%3Dpng" alt></p><p>文章的结尾，再讲一个故事吧。</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_png/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5l1iaBice2EYLlVicF9dXJsURRrkMr9cHYxZA53NjTKkZO7hibHhONFYJpGQ/640%3Fwx_fmt%3Dpng" alt></p><p>这幅画是描绘的是第二次鸦片战争当中的八里桥战役的，</p><p>该战当中，僧格林沁王爷率领“大清最后的铁骑”蒙古骑兵以三倍于敌的兵力对英法联军发动了主动突击，最终以尸横遍野、精锐尽丧的代价，换得了<strong>法军死3人、英军死2人</strong>的战果。</p><p>作为晚清最后的嫡系精锐，僧格林沁的蒙古骑兵作战不可谓不英勇，训练不可谓不有素。但问题是，他们苦练的那些技艺，什么弓马娴熟、镫里藏身，都是旧时代的技艺，当开花弹、连发枪、套筒刺刀等等等等革命性技术接连发生的时候，这些技艺很迅速的就被淘汰为了无用之技。</p><p>高度熟练这种技艺的个体是可悲的，因为他们终难有用武之地。而执着于这种训练和选拔，无法扭转的社会实体则更可悲，因为科技革命对他们并不公平**，一种新技术的演进对别人来说只是换个活法，而对他们却可能真的是灭顶之灾。**</p><p>但愿，我们执着的不是新的“镫里藏身”，但愿我们不会再迎面撞上一轮技术突变引发的快枪齐射。</p><p><strong><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_jpg/E3R2kpnIkmO6gOQPwXTL82AXhVE2rD5lFmtcFibic5ibtNfiaH5b6usb0w7LdR8EWKKAeO2nM1jWz8NTdPibJGGQiaBw/640%3Fwx_fmt%3Djpeg" alt></strong></p><p>全文完</p><p>本文7000字，感谢读完，愿对你有启发。</p><p>长文不易，喜欢请三连，多谢。</p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_jpg/E3R2kpnIkmNmAKjz5CEC7qFuymriaLZrFyzVxP9gFicCsJ4pplhHGkmUqbezuuvAbJcmzIvj3tpL2I1kckAhr9sA/640%3Fwx_fmt%3Djpeg" alt></p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_png/E3R2kpnIkmMpwsdziaN3oXSghicchb9l60EhWvS55NEOej3iasT9v6XibFrXiagPLyiavDfGrMACqv7oryvGkaAaSy1g/640%3Fwx_fmt%3Dpng" alt></p><p><img src="https://images.weserv.nl/?url=https%3A//mmbiz.qpic.cn/mmbiz_jpg/E3R2kpnIkmNmAKjz5CEC7qFuymriaLZrF680HYNnXKibbkNzLm7wXmXNbIbguO9LSKhPKEFxR5BtqibXRyOibyvyWw/640%3Fwx_fmt%3Djpeg" alt></p><h5><a href="https://www.digitalocean.com/?refcode=4351d40e44b2&utm_campaign=Referral_Invite&utm_medium=Referral_Program&utm_source=CopyPaste">最简单好用的 VPS,没有之一，注册立得 100 美金</a></h5></div><div class="col-lg-8 mx-auto block shadow"><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var t=document,e=t.createElement('script');e.async=!0,e.src='//pin-cong-jing-xuan.disqus.com/embed.js',e.setAttribute('data-timestamp',+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><div class="col-lg-8 mx-auto block shadow"><h3>See Also</h3><div class=container><div class=row><div class="mx-auto px-0"><div class="bg-white shadow block"><article class=mb-5><h2 class=h5><a class=text-dark href="https://project-gutenberg.github.io/Pincong/post/7b9485f16378607c6a3c4cf63aa02a3f/?utm_source=see_also&utm_medium=%25E6%2588%2591%25E4%25B8%258D%25E6%258B%2585%25E5%25BF%2583chatgpt%25E6%258A%25A2%25E6%2588%2591%25E9%25A5%25AD%25E7%25A2%2597%25E4%25BD%2586%25E6%2588%2591%25E6%258B%2585%25E5%25BF%2583%25E9%2582%25A3%25E4%25BB%25B6%25E6%259B%25B4%25E8%25A6%2581%25E5%2591%25BD%25E7%259A%2584%25E4%25BA%258B">被公众严重误解的公知“杜甫”</a></h2><p class=text-dark>收录于合集 #杂思 34个
对公知的批评最为激烈的莫过于著名思想家哈耶克：“这些家伙总喜欢故作高深，实则兜售观念的高手。”
这还不够，他接着咬牙切齿地骂道：”听到一些道听途说的知识之后，这帮家伙便到处唧唧歪歪，让人烦不胜烦。”
美国著名法律 …</p></article><article class=mb-5><h2 class=h5><a class=text-dark href="https://project-gutenberg.github.io/Pincong/post/c4ff4125517a772b571c709400e10ce9/?utm_source=see_also&utm_medium=%25E6%2588%2591%25E4%25B8%258D%25E6%258B%2585%25E5%25BF%2583chatgpt%25E6%258A%25A2%25E6%2588%2591%25E9%25A5%25AD%25E7%25A2%2597%25E4%25BD%2586%25E6%2588%2591%25E6%258B%2585%25E5%25BF%2583%25E9%2582%25A3%25E4%25BB%25B6%25E6%259B%25B4%25E8%25A6%2581%25E5%2591%25BD%25E7%259A%2584%25E4%25BA%258B">今年高校毕业生人数首破千万，媒体报道人工智能岗位毕业生月工资 23960 元，有哪些值得关注的信息？</a></h2><p class=text-dark>知乎用户 每日经济新闻​ 发表 得了吧。。
AI 行业要能够从业，基本上都是要名校 + 硕士出身的，这就已经很卷了。23960 元的工资大概率也是这些人拿的，其它人根本没机会。更高的工资一般都是名校 + 名师 + 博士 + 多文章，更加凤毛 …</p></article><article class=mb-5><h2 class=h5><a class=text-dark href="https://project-gutenberg.github.io/Pincong/post/0af45761db4e888742a1657c34c830cd/?utm_source=see_also&utm_medium=%25E6%2588%2591%25E4%25B8%258D%25E6%258B%2585%25E5%25BF%2583chatgpt%25E6%258A%25A2%25E6%2588%2591%25E9%25A5%25AD%25E7%25A2%2597%25E4%25BD%2586%25E6%2588%2591%25E6%258B%2585%25E5%25BF%2583%25E9%2582%25A3%25E4%25BB%25B6%25E6%259B%25B4%25E8%25A6%2581%25E5%2591%25BD%25E7%259A%2584%25E4%25BA%258B">人工智能就业前景越来越严峻了，你还在坚持吗？</a></h2><p class=text-dark>知乎用户 wei chris 发表 坚持那是肯定的，不坚持连饭都吃不上。
2019 年，我给公司面试了很多人，时间超过 1 个半小时的应该在 100 人以上，但是入职的不超过 5 个。结合我自己在公司内部的工作，基本上可以说，至少 …</p></article><article class=mb-5><h2 class=h5><a class=text-dark href="https://project-gutenberg.github.io/Pincong/post/a1c9220cd40b4deb50b26f975c43f357/?utm_source=see_also&utm_medium=%25E6%2588%2591%25E4%25B8%258D%25E6%258B%2585%25E5%25BF%2583chatgpt%25E6%258A%25A2%25E6%2588%2591%25E9%25A5%25AD%25E7%25A2%2597%25E4%25BD%2586%25E6%2588%2591%25E6%258B%2585%25E5%25BF%2583%25E9%2582%25A3%25E4%25BB%25B6%25E6%259B%25B4%25E8%25A6%2581%25E5%2591%25BD%25E7%259A%2584%25E4%25BA%258B">刘润：上海难题</a></h2><p class=text-dark>1、我同意一位我非常尊敬的老师的说法：上海问题，其实是一个“电车难题”。
2、什么是电车难题？电车难题，最初起源于英国哲学家菲利帕·富特1967年的一篇论文。经过多年迭代，我们今天看到的版本是这样的：
一辆电车在铁轨上行驶。在电车行进的方向 …</p></article><article class=mb-5><h2 class=h5><a class=text-dark href="https://project-gutenberg.github.io/Pincong/post/2973b6f894c97fd16575f0f3d64612d9/?utm_source=see_also&utm_medium=%25E6%2588%2591%25E4%25B8%258D%25E6%258B%2585%25E5%25BF%2583chatgpt%25E6%258A%25A2%25E6%2588%2591%25E9%25A5%25AD%25E7%25A2%2597%25E4%25BD%2586%25E6%2588%2591%25E6%258B%2585%25E5%25BF%2583%25E9%2582%25A3%25E4%25BB%25B6%25E6%259B%25B4%25E8%25A6%2581%25E5%2591%25BD%25E7%259A%2584%25E4%25BA%258B">“电车难题”不是防疫一刀切的借口</a></h2><p class=text-dark>电车难题，或许是知名度最高的思想实验之一：电车失控了，有两条轨道，一条轨上有五个孩子，另一条轨上有两个孩子，因为种种原因，他们都躲不开，如果你是司机，你会开往哪一边？
由于新冠疫情，电车难题在社交网络上被更频繁的引用，引用的目的，大多不是为 …</p></article></div></div></div></div></div></div></div></div></section><script>var i,images=document.getElementsByTagName("img");for(i=0;i<images.length;i++)images[i].className+="img-fluid w-100 mb-4"</script><footer class="py-4 bg-light border-top"><div class=container><div class="row justify-content-between align-items-center"><div class="col-lg-4 text-center text-lg-left mb-4 mb-lg-0"><a href=https://project-gutenberg.github.io/Pincong/><img src=https://project-gutenberg.github.io/Pincong//images/logo.png class=img-fluid alt=品葱*精选></a></div><div class="col-lg-4 text-center mb-4 mb-lg-0"><ul class="list-inline mb-0"></ul></div><div class="col-lg-4 text-lg-right text-center mb-4 mb-lg-0"><ul class="list-inline social-icon mb-0"><li class=list-inline-item><a href=https://pincong.rocks/><i class=ti-home></i></a></li><li class=list-inline-item><a href=https://github.com/Project-Gutenberg/Pincong><i class=ti-github></i></a></li></ul></div><div class="col-12 text-center mt-4"><span></span></div></div></div></footer><script src=../../plugins/jQuery/jquery.min.js></script>
<script src=../../plugins/bootstrap/bootstrap.min.js></script>
<script src=../../plugins/search/fuse.min.js></script>
<script src=../../plugins/search/mark.js></script>
<script src=../../plugins/search/search.js></script>
<script src=https://project-gutenberg.github.io/Pincong/js/script.min.js></script>
<script>(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)})(window,document,'script','//www.google-analytics.com/analytics.js','ga'),ga('create','UA-151212685-6','auto'),ga('send','pageview')</script></body></html>